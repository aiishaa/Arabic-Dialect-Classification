{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiishaa/Arabic-Dialect-Classification/blob/main/Data_pre_processing_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1wp60bf5Q2T",
        "outputId": "ef57a12e-b333-48ba-871c-7dc9dc805c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.14-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from pyarabic) (1.15.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.14\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYkZDbUN2HyK",
        "outputId": "002ca5aa-6ff6-4d31-d6b6-b4da7fdeaabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    ['@Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØº...\n",
            "1    ['@7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .....\n",
            "2                  ['@KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ']\n",
            "3       ['@HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ðŸ’']\n",
            "4               ['@hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ðŸŒ¸ðŸŒº']\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from snowballstemmer import stemmer\n",
        "from nltk.corpus import stopwords\n",
        "import pyarabic.araby as araby\n",
        "import unicodedata\n",
        "df = pd.read_csv('/content/drive/MyDrive/texts.csv')\n",
        "print(df['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhoQVUID2XEy",
        "outputId": "1929d505-eea9-4132-b594-3307acc54c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162000\n",
            "[ 4 14  4 ... 15 15 15]\n"
          ]
        }
      ],
      "source": [
        "# take only 9000 samples from each class to speed the computations\n",
        "nrows = len(df)\n",
        "sample_size = 9000\n",
        "df = df.groupby('dialect').apply(lambda x: x.sample(sample_size))\n",
        "print(len(df))\n",
        "\n",
        "random_df = df.sample(frac=1)\n",
        "id = random_df[\"id\"].to_numpy()\n",
        "dialect = random_df[\"dialect\"].to_numpy()\n",
        "print(dialect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A3P0p5fQ3Gvl"
      },
      "outputs": [],
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r' ',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wx1GcgBy3LE_"
      },
      "outputs": [],
      "source": [
        "def tokenize(strr):\n",
        "  a_list = nltk.word_tokenize(strr)\n",
        "  return a_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f2P_yuZZ3Z3G",
        "outputId": "3db4d3f0-c40d-4a59-900b-de0ecbfeb4be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ø³Ù…ÙŠ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#run check that the stemmer works\n",
        "ar_stemmer = stemmer(\"arabic\")\n",
        "ar_stemmer.stemWord(\"ÙØ³Ù…ÙŠØªÙ…ÙˆÙ‡Ø§\") #exmaple to check that it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y6KPXPOn3lMA"
      },
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "    result = []\n",
        "    output = re.sub(r'[0-9A-Za-z\\\\ØŸ]', ' ' , sentence)\n",
        "    output = output.rstrip()\n",
        "    # Normalize unicode encoding\n",
        "    output = unicodedata.normalize('NFC', output)\n",
        "\n",
        "    # Remove '@name'\n",
        "    output = re.sub(r'(@.*?)[\\s]', ' ', output)\n",
        "\n",
        "    # remove emojis\n",
        "    output =  deEmojify(output)\n",
        "\n",
        "    # Remove URLs\n",
        "    output = re.sub(r'http\\S+', ' ', output)\n",
        "\n",
        "    # Remove trailing whitespace and new lines\n",
        "    output = re.sub(r'\\s+', ' ', output).strip()\n",
        "    output = output.replace('\\\\n', ' ').replace('\\n', ' ')\n",
        "  \n",
        "    # Remove special characters from the string\n",
        "    pattern = r'[' + string.punctuation + ']'\n",
        "    output = re.sub(pattern, ' ', output)\n",
        "    output = ''.join(c for c in output if not unicodedata.category(c).startswith('P'))\n",
        "\n",
        "    # tokenize the sentence\n",
        "    result.append(output)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE7pBjdn3s0w",
        "outputId": "06e0f384-67d3-4e76-bf2a-d6317baf92f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\") #arabic stopwords are not biult-in, so we find them by calling a set object\n",
        "stopwords_list = stopwords.words('arabic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa3M17lc30Gm",
        "outputId": "10a3853b-fa87-44ee-a0e5-8d5d3d010cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162000\n"
          ]
        }
      ],
      "source": [
        "def cleanArabicText(sentence):\n",
        "    words = preprocess(sentence)\n",
        "    stopwords_list = stopwords.words('arabic')\n",
        "    words = [ar_stemmer.stemWord(araby.strip_diacritics(w)) for w in words if araby.strip_diacritics(w) not in stopwords_list and len(w) > 1]\n",
        "    return ' '.join(words)\n",
        "# End of Func\n",
        "print(len(random_df))\n",
        "random_df['text_cleaned'] = random_df['text'].apply(cleanArabicText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrPjoKwP4eyA",
        "outputId": "ea364361-c135-4d2e-d355-e478c75c28f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ù…ØºÙŠØ¨  ÙˆÙ„Ø§ Ø¹Ù†Ø¯Ù‡ ØºÙŠØ± Ø¹ØµØ§ÙŠØµ Ù…Ù† Ø§Ù„Ù„ÙŠ ÙŠØ¯Ø®Ù†Ù‡ \n"
          ]
        }
      ],
      "source": [
        "# save preprocessed texts in a csv file for training\n",
        "print(cleanArabicText('Ù…ØºÙŠØ¨ ØŒØŒ ÙˆÙ„Ø§ Ø¹Ù†Ø¯Ù‡ ØºÙŠØ± Ø¹ØµØ§ÙŠØµ Ù…Ù† Ø§Ù„Ù„ÙŠ ÙŠØ¯Ø®Ù†Ù‡ ØŒØŒ'))\n",
        "random_df.to_csv('/content/drive/MyDrive/cleaned_texts.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data pre-processing .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MGBEiRdKmSL201Ai464Xmf0VFFrg20US",
      "authorship_tag": "ABX9TyNDvYabrgPe7ZhFwF61xKUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}